{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'à',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Analysis of stopwords\n",
    "stopwords.words('french') #lists all the stopwords in french"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis of CMU wordlist\n",
    "import nltk\n",
    "entries=nltk.corpus.cmudict.entries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133737"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entries) #finding the no. of entries in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('abkhazia', ['AE0', 'B', 'K', 'HH', 'AE1', 'Z', 'Y', 'AH0'])\n",
      "('abkhazian', ['AE0', 'B', 'K', 'HH', 'AA1', 'Z', 'IY0', 'AH0', 'N'])\n",
      "('abkhazian', ['AE0', 'B', 'K', 'HH', 'AE1', 'Z', 'IY0', 'AH0', 'N'])\n",
      "('abkhazian', ['AE0', 'B', 'K', 'HH', 'AA1', 'Z', 'Y', 'AH0', 'N'])\n",
      "('abkhazian', ['AE0', 'B', 'K', 'HH', 'AE1', 'Z', 'Y', 'AH0', 'N'])\n",
      "('abkhazians', ['AE0', 'B', 'K', 'HH', 'AA1', 'Z', 'IY0', 'AH0', 'N', 'Z'])\n",
      "('abkhazians', ['AE0', 'B', 'K', 'HH', 'AE1', 'Z', 'IY0', 'AH0', 'N', 'Z'])\n",
      "('ablaze', ['AH0', 'B', 'L', 'EY1', 'Z'])\n",
      "('able', ['EY1', 'B', 'AH0', 'L'])\n",
      "('able-bodied', ['EY1', 'B', 'AH0', 'L', 'B', 'AA1', 'D', 'IY0', 'D'])\n",
      "('abled', ['EY1', 'B', 'AH0', 'L', 'D'])\n",
      "('abler', ['EY1', 'B', 'AH0', 'L', 'ER0'])\n",
      "('abler', ['EY1', 'B', 'L', 'ER0'])\n",
      "('ables', ['EY1', 'B', 'AH0', 'L', 'Z'])\n",
      "('ablest', ['EY1', 'B', 'AH0', 'L', 'S', 'T'])\n",
      "('ablest', ['EY1', 'B', 'L', 'AH0', 'S', 'T'])\n",
      "('abloom', ['AH0', 'B', 'L', 'UW1', 'M'])\n",
      "('ably', ['EY1', 'B', 'L', 'IY0'])\n",
      "('abner', ['AE1', 'B', 'N', 'ER0'])\n",
      "('abney', ['AE1', 'B', 'N', 'IY0'])\n",
      "('abnormal', ['AE0', 'B', 'N', 'AO1', 'R', 'M', 'AH0', 'L'])\n",
      "('abnormalities', ['AE2', 'B', 'N', 'AO0', 'R', 'M', 'AE1', 'L', 'AH0', 'T', 'IY0', 'Z'])\n",
      "('abnormality', ['AE2', 'B', 'N', 'AO0', 'R', 'M', 'AE1', 'L', 'AH0', 'T', 'IY0'])\n",
      "('abnormally', ['AE0', 'B', 'N', 'AO1', 'R', 'M', 'AH0', 'L', 'IY0'])\n",
      "('abo', ['AA1', 'B', 'OW0'])\n",
      "('aboard', ['AH0', 'B', 'AO1', 'R', 'D'])\n",
      "('abode', ['AH0', 'B', 'OW1', 'D'])\n",
      "('abohalima', ['AE0', 'B', 'AH0', 'HH', 'AH0', 'L', 'IY1', 'M', 'AH0'])\n",
      "('abolish', ['AH0', 'B', 'AA1', 'L', 'IH0', 'SH'])\n",
      "('abolished', ['AH0', 'B', 'AA1', 'L', 'IH0', 'SH', 'T'])\n",
      "('abolishes', ['AH0', 'B', 'AA1', 'L', 'IH0', 'SH', 'IH0', 'Z'])\n",
      "('abolishing', ['AH0', 'B', 'AA1', 'L', 'IH0', 'SH', 'IH0', 'NG'])\n",
      "('abolition', ['AE2', 'B', 'AH0', 'L', 'IH1', 'SH', 'AH0', 'N'])\n",
      "('abolitionism', ['AE2', 'B', 'AH0', 'L', 'IH1', 'SH', 'AH0', 'N', 'IH2', 'Z', 'AH0', 'M'])\n",
      "('abolitionist', ['AE2', 'B', 'AH0', 'L', 'IH1', 'SH', 'AH0', 'N', 'AH0', 'S', 'T'])\n",
      "('abolitionists', ['AE2', 'B', 'AH0', 'L', 'IH1', 'SH', 'AH0', 'N', 'AH0', 'S', 'T', 'S'])\n",
      "('abolitionists', ['AE2', 'B', 'AH0', 'L', 'IH1', 'SH', 'AH0', 'N', 'AH0', 'S'])\n",
      "('abominable', ['AH0', 'B', 'AA1', 'M', 'AH0', 'N', 'AH0', 'B', 'AH0', 'L'])\n",
      "('abomination', ['AH0', 'B', 'AA2', 'M', 'AH0', 'N', 'EY1', 'SH', 'AH0', 'N'])\n",
      "('abood', ['AH0', 'B', 'UW1', 'D'])\n",
      "('aboodi', ['AH0', 'B', 'UW1', 'D', 'IY0'])\n",
      "('abor', ['AH0', 'B', 'AO1', 'R'])\n",
      "('aboriginal', ['AE2', 'B', 'ER0', 'IH1', 'JH', 'AH0', 'N', 'AH0', 'L'])\n",
      "('aborigine', ['AE2', 'B', 'ER0', 'IH1', 'JH', 'AH0', 'N', 'IY0'])\n",
      "('aborigines', ['AE2', 'B', 'ER0', 'IH1', 'JH', 'AH0', 'N', 'IY0', 'Z'])\n",
      "('aborn', ['AH0', 'B', 'AO1', 'R', 'N'])\n",
      "('abort', ['AH0', 'B', 'AO1', 'R', 'T'])\n",
      "('aborted', ['AH0', 'B', 'AO1', 'R', 'T', 'IH0', 'D'])\n",
      "('abortifacient', ['AH0', 'B', 'AO2', 'R', 'T', 'AH0', 'F', 'EY1', 'SH', 'AH0', 'N', 'T'])\n",
      "('abortifacients', ['AH0', 'B', 'AO2', 'R', 'T', 'AH0', 'F', 'EY1', 'SH', 'AH0', 'N', 'T', 'S'])\n"
     ]
    }
   ],
   "source": [
    "for entry in entries[200:250]: #extracting entries within a given range\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.02')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#analysis of wordnet\n",
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('railcar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'auto', 'automobile', 'machine', 'motorcar']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The synonyms are grouped into synsets with short definitions and usage examples. We get an id of subsets using the wn.synsets() functionality. It has an optional pos argument which lets you constrain the part of speech of the word. \n",
    "wn.synset('car.n.01').lemma_names()\n",
    "#A synset is identified with a 3-part name of the form: word.pos.nn. We can extract all those synsets that have a particular synset identifier using the lemma_names() functionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp pipeline\n",
    "import nltk\n",
    "texts=\"\"\"\"Robert John Downey Jr. (born April 4, 1965) is an American actor, producer, and singer. His career has been characterized by critical and popular success in his youth, followed by a period of substance abuse and legal troubles, before a resurgence of commercial success in middle age. In 2008, Downey was named by Time magazine among the 100 most influential people in the world, and from 2013 to 2015, he was listed by Forbes as Hollywood's highest-paid actor. His films have grossed over $14.4 billion worldwide, making him the second highest-grossing box-office star of all time.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('``', '``')]\n",
      "[('R', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('b', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('J', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('D', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('w', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('y', 'NN')]\n",
      "[('J', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('.', '.')]\n",
      "[('(', '(')]\n",
      "[('b', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('A', 'DT')]\n",
      "[('p', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('4', 'CD')]\n",
      "[(',', ',')]\n",
      "[('1', 'CD')]\n",
      "[('9', 'CD')]\n",
      "[('6', 'CD')]\n",
      "[('5', 'CD')]\n",
      "[(')', ')')]\n",
      "[('i', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('n', 'NN')]\n",
      "[('A', 'DT')]\n",
      "[('m', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('a', 'DT')]\n",
      "[('n', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('c', 'NNS')]\n",
      "[('t', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[(',', ',')]\n",
      "[('p', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('e', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[(',', ',')]\n",
      "[('a', 'DT')]\n",
      "[('n', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('g', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('.', '.')]\n",
      "[('H', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('a', 'DT')]\n",
      "[('r', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('s', 'NN')]\n",
      "[('b', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('h', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('r', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('c', 'NNS')]\n",
      "[('t', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('z', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('b', 'NN')]\n",
      "[('y', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('r', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('a', 'DT')]\n",
      "[('l', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('n', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('p', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('p', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('r', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('c', 'NNS')]\n",
      "[('e', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('y', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[(',', ',')]\n",
      "[('f', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('w', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('b', 'NN')]\n",
      "[('y', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('p', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('f', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('b', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('n', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('e', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('b', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('n', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('g', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('l', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('b', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[(',', ',')]\n",
      "[('b', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('f', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('r', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('g', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('e', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('f', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('o', 'NN')]\n",
      "[('m', 'NN')]\n",
      "[('m', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('i', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('l', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('c', 'NNS')]\n",
      "[('e', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('m', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('g', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('.', '.')]\n",
      "[('I', 'PRP')]\n",
      "[('n', 'NN')]\n",
      "[('2', 'CD')]\n",
      "[('0', 'CD')]\n",
      "[('0', 'CD')]\n",
      "[('8', 'CD')]\n",
      "[(',', ',')]\n",
      "[('D', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('w', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('y', 'NN')]\n",
      "[('w', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('s', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('m', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('b', 'NN')]\n",
      "[('y', 'NN')]\n",
      "[('T', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('m', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('m', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('g', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('z', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('m', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('g', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('1', 'CD')]\n",
      "[('0', 'CD')]\n",
      "[('0', 'CD')]\n",
      "[('m', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('f', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('u', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('l', 'NN')]\n",
      "[('p', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('p', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('w', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[(',', ',')]\n",
      "[('a', 'DT')]\n",
      "[('n', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('f', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('m', 'NN')]\n",
      "[('2', 'CD')]\n",
      "[('0', 'CD')]\n",
      "[('1', 'CD')]\n",
      "[('3', 'CD')]\n",
      "[('t', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('2', 'CD')]\n",
      "[('0', 'CD')]\n",
      "[('1', 'CD')]\n",
      "[('5', 'CD')]\n",
      "[(',', ',')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('w', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('s', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('b', 'NN')]\n",
      "[('y', 'NN')]\n",
      "[('F', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('b', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('s', 'NN')]\n",
      "[('H', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('y', 'NN')]\n",
      "[('w', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[(\"'\", \"''\")]\n",
      "[('s', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('g', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('-', ':')]\n",
      "[('p', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('i', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('c', 'NNS')]\n",
      "[('t', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('.', '.')]\n",
      "[('H', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('f', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('m', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('v', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('g', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('v', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('$', '$')]\n",
      "[('1', 'CD')]\n",
      "[('4', 'CD')]\n",
      "[('.', '.')]\n",
      "[('4', 'CD')]\n",
      "[('b', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('w', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('w', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[(',', ',')]\n",
      "[('m', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('k', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('g', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('m', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('o', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('d', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('g', 'NN')]\n",
      "[('h', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('-', ':')]\n",
      "[('g', 'NN')]\n",
      "[('r', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('n', 'NN')]\n",
      "[('g', 'NN')]\n",
      "[('b', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('x', 'NN')]\n",
      "[('-', ':')]\n",
      "[('o', 'NN')]\n",
      "[('f', 'NN')]\n",
      "[('f', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('c', 'NNS')]\n",
      "[('e', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('r', 'NN')]\n",
      "[('o', 'NN')]\n",
      "[('f', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('l', 'NN')]\n",
      "[('l', 'NN')]\n",
      "[('t', 'NN')]\n",
      "[('i', 'NN')]\n",
      "[('m', 'NN')]\n",
      "[('e', 'NN')]\n",
      "[('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for text in texts:#tokenising the words in text and provide POS tags\n",
    "    sentences=nltk.sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        words=nltk.word_tokenize(sentence)\n",
    "        tagged_words=nltk.pos_tag(words)\n",
    "        print(tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['After',\n",
       " 'years',\n",
       " 'of',\n",
       " 'rebuilding',\n",
       " 'OTHER',\n",
       " 'nations',\n",
       " ',',\n",
       " ':D',\n",
       " 'we',\n",
       " 'are',\n",
       " 'finally',\n",
       " 'rebuilding',\n",
       " 'OUR',\n",
       " 'nation',\n",
       " '.',\n",
       " 'We',\n",
       " 'are',\n",
       " 'finally',\n",
       " 'putting',\n",
       " 'AMERICA',\n",
       " 'FIRST',\n",
       " '!',\n",
       " '#KAG2020']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tweets\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "text='After years of rebuilding OTHER nations,:D we are finally rebuilding OUR nation. We are finally putting AMERICA FIRST! #KAG2020'\n",
    "twtkn=TweetTokenizer()\n",
    "twtkn.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FREQUENCY DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n"
     ]
    }
   ],
   "source": [
    "#We extract the words that are part of the specified category using Brown corpus \n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "news_text=brown.words(categories='news')\n",
    "fdist=nltk.FreqDist(w.lower() for w in news_text)\n",
    "print(news_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "modals=['can','could','may','might','must','will']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can: 94 could: 87 may: 93 might: 38 must: 53 will: 389 "
     ]
    }
   ],
   "source": [
    "for a in modals:\n",
    "    print(a +':',fdist[a],end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 13112 samples and 100554 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
